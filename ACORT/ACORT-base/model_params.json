{
  "breakdown": {
    "att_embed.0.bias": 512,
    "att_embed.0.weight": 1048576,
    "model.decoder.layers.0.feed_forward.w_1.bias": 2048,
    "model.decoder.layers.0.feed_forward.w_1.weight": 1048576,
    "model.decoder.layers.0.feed_forward.w_2.bias": 512,
    "model.decoder.layers.0.feed_forward.w_2.weight": 1048576,
    "model.decoder.layers.0.self_attn.linears.0.bias": 512,
    "model.decoder.layers.0.self_attn.linears.0.weight": 262144,
    "model.decoder.layers.0.self_attn.linears.1.bias": 512,
    "model.decoder.layers.0.self_attn.linears.1.weight": 262144,
    "model.decoder.layers.0.self_attn.linears.2.bias": 512,
    "model.decoder.layers.0.self_attn.linears.2.weight": 262144,
    "model.decoder.layers.0.src_attn.linears.0.bias": 512,
    "model.decoder.layers.0.src_attn.linears.0.weight": 262144,
    "model.decoder.layers.0.src_attn.linears.1.bias": 512,
    "model.decoder.layers.0.src_attn.linears.1.weight": 262144,
    "model.decoder.layers.0.src_attn.linears.2.bias": 512,
    "model.decoder.layers.0.src_attn.linears.2.weight": 262144,
    "model.decoder.layers.0.sublayer.0.norm.a_2": 512,
    "model.decoder.layers.0.sublayer.0.norm.b_2": 512,
    "model.decoder.layers.0.sublayer.1.norm.a_2": 512,
    "model.decoder.layers.0.sublayer.1.norm.b_2": 512,
    "model.decoder.layers.0.sublayer.2.norm.a_2": 512,
    "model.decoder.layers.0.sublayer.2.norm.b_2": 512,
    "model.decoder.layers.3.feed_forward.w_1.bias": 2048,
    "model.decoder.layers.3.feed_forward.w_1.weight": 1048576,
    "model.decoder.layers.3.feed_forward.w_2.bias": 512,
    "model.decoder.layers.3.feed_forward.w_2.weight": 1048576,
    "model.decoder.layers.3.self_attn.linears.0.bias": 512,
    "model.decoder.layers.3.self_attn.linears.0.weight": 262144,
    "model.decoder.layers.3.self_attn.linears.1.bias": 512,
    "model.decoder.layers.3.self_attn.linears.1.weight": 262144,
    "model.decoder.layers.3.self_attn.linears.2.bias": 512,
    "model.decoder.layers.3.self_attn.linears.2.weight": 262144,
    "model.decoder.layers.3.src_attn.linears.0.bias": 512,
    "model.decoder.layers.3.src_attn.linears.0.weight": 262144,
    "model.decoder.layers.3.src_attn.linears.1.bias": 512,
    "model.decoder.layers.3.src_attn.linears.1.weight": 262144,
    "model.decoder.layers.3.src_attn.linears.2.bias": 512,
    "model.decoder.layers.3.src_attn.linears.2.weight": 262144,
    "model.decoder.layers.3.sublayer.0.norm.a_2": 512,
    "model.decoder.layers.3.sublayer.0.norm.b_2": 512,
    "model.decoder.layers.3.sublayer.1.norm.a_2": 512,
    "model.decoder.layers.3.sublayer.1.norm.b_2": 512,
    "model.decoder.layers.3.sublayer.2.norm.a_2": 512,
    "model.decoder.layers.3.sublayer.2.norm.b_2": 512,
    "model.decoder.norm.a_2": 512,
    "model.decoder.norm.b_2": 512,
    "model.encoder.layers.0.feed_forward.w_1.bias": 2048,
    "model.encoder.layers.0.feed_forward.w_1.weight": 1048576,
    "model.encoder.layers.0.feed_forward.w_2.bias": 512,
    "model.encoder.layers.0.feed_forward.w_2.weight": 1048576,
    "model.encoder.layers.0.self_attn.WGs.0.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.0.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.1.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.1.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.2.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.2.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.3.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.3.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.4.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.4.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.5.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.5.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.6.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.6.weight": 64,
    "model.encoder.layers.0.self_attn.WGs.7.bias": 1,
    "model.encoder.layers.0.self_attn.WGs.7.weight": 64,
    "model.encoder.layers.0.self_attn.linears.0.bias": 512,
    "model.encoder.layers.0.self_attn.linears.0.weight": 262144,
    "model.encoder.layers.0.self_attn.linears.1.bias": 512,
    "model.encoder.layers.0.self_attn.linears.1.weight": 262144,
    "model.encoder.layers.0.self_attn.linears.2.bias": 512,
    "model.encoder.layers.0.self_attn.linears.2.weight": 262144,
    "model.encoder.layers.0.sublayer.0.norm.a_2": 512,
    "model.encoder.layers.0.sublayer.0.norm.b_2": 512,
    "model.encoder.layers.0.sublayer.1.norm.a_2": 512,
    "model.encoder.layers.0.sublayer.1.norm.b_2": 512,
    "model.encoder.layers.3.feed_forward.w_1.bias": 2048,
    "model.encoder.layers.3.feed_forward.w_1.weight": 1048576,
    "model.encoder.layers.3.feed_forward.w_2.bias": 512,
    "model.encoder.layers.3.feed_forward.w_2.weight": 1048576,
    "model.encoder.layers.3.self_attn.WGs.0.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.0.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.1.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.1.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.2.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.2.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.3.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.3.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.4.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.4.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.5.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.5.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.6.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.6.weight": 64,
    "model.encoder.layers.3.self_attn.WGs.7.bias": 1,
    "model.encoder.layers.3.self_attn.WGs.7.weight": 64,
    "model.encoder.layers.3.self_attn.linears.0.bias": 512,
    "model.encoder.layers.3.self_attn.linears.0.weight": 262144,
    "model.encoder.layers.3.self_attn.linears.1.bias": 512,
    "model.encoder.layers.3.self_attn.linears.1.weight": 262144,
    "model.encoder.layers.3.self_attn.linears.2.bias": 512,
    "model.encoder.layers.3.self_attn.linears.2.weight": 262144,
    "model.encoder.layers.3.sublayer.0.norm.a_2": 512,
    "model.encoder.layers.3.sublayer.0.norm.b_2": 512,
    "model.encoder.layers.3.sublayer.1.norm.a_2": 512,
    "model.encoder.layers.3.sublayer.1.norm.b_2": 512,
    "model.encoder.norm.a_2": 512,
    "model.encoder.norm.b_2": 512,
    "model.generator.proj.bias": 771,
    "model.generator.proj.weight": 394752,
    "model.tgt_embed.0.lut.weight": 394752
  },
  "total": 14979347,
  "trainable params": 14979347
}